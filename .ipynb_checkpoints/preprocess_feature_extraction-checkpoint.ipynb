{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7799a816-d05a-45c7-8f5d-2b5c1c51738a",
   "metadata": {},
   "source": [
    "This Jupyter Notebook is designed to preprocess and extract features from time-series data. The notebook loads datasets, processes them to remove any invalid data, and extracts meaningful features using both custom functions and the tsfresh library. The processed data is then prepared for downstream machine learning tasks.\n",
    "\n",
    "Key Steps in the Notebook\n",
    "1. Importing Necessary Libraries\n",
    "The notebook begins by importing essential Python libraries such as:\n",
    "Pandas: For data manipulation and analysis.\n",
    "NumPy: For numerical operations.\n",
    "tsfresh: For time-series feature extraction.\n",
    "datetime and timedelta: For handling time-related operations.\n",
    "ast: For safely evaluating expressions from the data.\n",
    "\n",
    "3. Loading the Data\n",
    "The notebook loads time-series data from CSV files corresponding to five different blade feeding stations:\n",
    "Station 1: BIC_Blade_Feeding_Station1_full.csv\n",
    "Station 2: BIC_Blade_Feeding_Station2_full.csv\n",
    "Station 3: BIC_Blade_Feeding_Station3_full.csv\n",
    "Station 4: BIC_Blade_Feeding_Station4_full.csv\n",
    "Station 5: BIC_Blade_Feeding_Station5_full.csv\n",
    "\n",
    "3. Data Cleaning\n",
    "Removing Rows with Empty Arrays:\n",
    "The function remove_empty_array_rows identifies and removes rows containing empty arrays across various columns. This step ensures that subsequent feature extraction processes do not encounter errors due to invalid data.\n",
    "\n",
    "5. Feature Extraction\n",
    "Custom Feature Extraction:\n",
    "The notebook defines several custom functions to calculate various statistical features from the time-series data, such as abs_energy, mean, maximum, and more. These functions are applied to each column of the data to extract relevant features.\n",
    "\n",
    "Using tsfresh for Feature Extraction:\n",
    "The tsfresh library is utilized to extract comprehensive features from the time-series data. This library automates the extraction of numerous features commonly used in time-series analysis.\n",
    "\n",
    "5. Processing Individual Stations\n",
    "The notebook processes each station's dataset individually using the process_station function, which:\n",
    "    Calculates time differences between specific start and end times.\n",
    "    Safely evaluates string representations of lists in the data.\n",
    "    Expands rows based on timestamp increments for better feature extraction.\n",
    "    Combines the extracted features with additional data columns.\n",
    "6. Merging and Saving Data\n",
    "After feature extraction, the notebook combines the features with the original data columns like time differences and station identifiers.\n",
    "The processed data for each station is then prepared for further analysis or modeling.\n",
    "\n",
    "8. Final Dataframe Construction\n",
    "The notebook constructs a final DataFrame for each station, aligning indices and merging feature data with additional columns.\n",
    "The resulting DataFrames are ready for use in machine learning models or other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "709fa193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tsfresh.feature_extraction import extract_features, MinimalFCParameters, ComprehensiveFCParameters\n",
    "from datetime import datetime, timedelta\n",
    "import ast\n",
    "from tsfresh import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cf5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('BIC_Blade_Feeding_Station1_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e79720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137186, 27)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb5585b-7886-44f4-9f88-b5a84a8352a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_empty_array_rows(df):\n",
    "    \"\"\"\n",
    "    This function removes rows that contain empty arrays from a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame with rows containing empty arrays removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_empty_array(value):\n",
    "        try:\n",
    "            # Try to parse the value as a literal (e.g., list)\n",
    "            parsed_value = ast.literal_eval(value)\n",
    "            # Check if it's a list and if it's empty\n",
    "            return isinstance(parsed_value, list) and len(parsed_value) == 0\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If it fails to parse, it's not a list\n",
    "            return False\n",
    "    \n",
    "    # Identify the rows with empty arrays\n",
    "    rows_with_empty_arrays = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        empty_array_rows = df[df[column].apply(is_empty_array)].index.tolist()\n",
    "        rows_with_empty_arrays.extend(empty_array_rows)\n",
    "    \n",
    "    # Remove duplicates (if the same row has empty arrays in multiple columns)\n",
    "    rows_with_empty_arrays = list(set(rows_with_empty_arrays))\n",
    "    \n",
    "    # Remove the rows from the DataFrame\n",
    "    df_cleaned = df.drop(rows_with_empty_arrays)\n",
    "    \n",
    "    # Reset the index of the cleaned DataFrame\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Display the number of rows removed\n",
    "    print(f\"Removed {len(rows_with_empty_arrays)} rows with empty arrays.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8f9cf6-625b-44a3-8898-c01b292874fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 614 rows with empty arrays.\n"
     ]
    }
   ],
   "source": [
    "df1= remove_empty_array_rows(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d48aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_station(df, station_id):\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(['Unnamed: 0', 'razor_id', 'datetime_cam_start', 'datetime_cam_end', 'cam_mov_pos', 'cam_mov_id'], axis=1)\n",
    "    \n",
    "    # Define the calculate_time_difference function\n",
    "    def calculate_time_difference(df, start_col, end_col):\n",
    "        df[start_col] = pd.to_datetime(df[start_col])\n",
    "        df[end_col] = pd.to_datetime(df[end_col])\n",
    "        df['required_time'] = df[end_col] - df[start_col]\n",
    "        new_col_name = f'{start_col}_to_{end_col}_milliseconds'\n",
    "        df[new_col_name] = df['required_time'].dt.total_seconds() * 1000\n",
    "        df = df.drop(['required_time'], axis=1)\n",
    "        return df\n",
    "    \n",
    "    # List of tuples with start and end datetime column pairs\n",
    "    datetime_columns = [('datetime_cl_start', 'datetime_cl_end'), \n",
    "                        ('datetime_ch_start', 'datetime_ch_end'),\n",
    "                        ('datetime_ph_start', 'datetime_ph_end'),\n",
    "                        ('datetime_pl_start', 'datetime_pl_end'),\n",
    "                        ('datetime_mov_start', 'datetime_mov_end')]\n",
    "    \n",
    "    # Calculate time differences for each pair of datetime columns\n",
    "    for start_col, end_col in datetime_columns:\n",
    "        df = calculate_time_difference(df, start_col, end_col)\n",
    "\n",
    "    # Define the safe_literal_eval function\n",
    "    def safe_literal_eval(val):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return val\n",
    "\n",
    "    # Apply the safe_literal_eval function to all specified columns\n",
    "    columns_to_parse = [\n",
    "        'carb_pos_low', 'carb_val_low',\n",
    "        'carb_pos_high', 'carb_val_high',\n",
    "        'pp_pos_high', 'pp_val_high',\n",
    "        'pp_pos_low', 'pp_val_low',\n",
    "    ]\n",
    "    \n",
    "    for col in columns_to_parse:\n",
    "        df[col] = df[col].apply(safe_literal_eval)\n",
    "\n",
    "    # Function to expand rows based on a single column\n",
    "    def expand_single_column(df, column, datetime_start_col, datetime_end_col):\n",
    "        expanded_rows = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            datetime_start = row[datetime_start_col]\n",
    "            datetime_end = row[datetime_end_col]\n",
    "\n",
    "            list_length = len(row[column])\n",
    "            if list_length > 1:\n",
    "                time_increment = (datetime_end - datetime_start) / (list_length - 1)\n",
    "            else:\n",
    "                time_increment = timedelta(0)\n",
    "\n",
    "            for i, value in enumerate(row[column]):\n",
    "                timestamp = datetime_start + i * time_increment\n",
    "                expanded_rows.append([idx, timestamp, value])\n",
    "\n",
    "        expanded_df = pd.DataFrame(expanded_rows, columns=['id', 'time', column])\n",
    "        \n",
    "        return expanded_df\n",
    "\n",
    "    # Define custom settings for feature extraction\n",
    "    custom_settings = {\n",
    "        \"maximum\": None,\n",
    "        \"mean\": None,\n",
    "        \"minimum\": None,\n",
    "        \"standard_deviation\": None,\n",
    "        \"variance\": None\n",
    "    }\n",
    "\n",
    "\n",
    "    # Function to process and extract features for a single column\n",
    "    def process_and_extract(df, column, datetime_start_col, datetime_end_col):\n",
    "        expanded_df = expand_single_column(df, column, datetime_start_col, datetime_end_col)\n",
    "\n",
    "        # Fill NaN values with zeros\n",
    "        # expanded_df = expanded_df.fillna(0)\n",
    "\n",
    "        extracted_features = extract_features(\n",
    "            expanded_df, column_id='id', column_sort='time', default_fc_parameters=custom_settings\n",
    "        )\n",
    "        return extracted_features\n",
    "\n",
    "    # Process each column with its respective datetime columns\n",
    "    carb_pos_low_features = process_and_extract(df, 'carb_pos_low', 'datetime_cl_start', 'datetime_cl_end')\n",
    "    carb_val_low_features = process_and_extract(df, 'carb_val_low', 'datetime_cl_start', 'datetime_cl_end')\n",
    "\n",
    "    # carb_pos_high_features = process_and_extract(df, 'carb_pos_high', 'datetime_ch_start', 'datetime_ch_end')\n",
    "    # carb_val_high_features = process_and_extract(df, 'carb_val_high', 'datetime_cl_start', 'datetime_cl_end')\n",
    "\n",
    "    # pp_pos_high_features = process_and_extract(df, 'pp_pos_high', 'datetime_ph_start', 'datetime_ph_end')\n",
    "    # pp_val_high_features = process_and_extract(df, 'pp_val_high', 'datetime_ph_start', 'datetime_ph_end')\n",
    "    \n",
    "    pp_pos_low_features = process_and_extract(df, 'pp_pos_low', 'datetime_pl_start', 'datetime_pl_end')\n",
    "    pp_val_low_features = process_and_extract(df, 'pp_val_low', 'datetime_pl_start', 'datetime_pl_end')\n",
    "    \n",
    "    # mov_pos_features = process_and_extract(df, 'mov_pos', 'datetime_mov_start', 'datetime_mov_end')\n",
    "\n",
    "\n",
    "    # Combine all features\n",
    "    all_features = pd.concat([carb_pos_low_features, carb_val_low_features, pp_pos_low_features, pp_val_low_features], axis=1)\n",
    "\n",
    "    # List of columns to merge from df\n",
    "    columns_to_merge = [\n",
    "        # 'datetime_cl_start_to_datetime_cl_end_milliseconds',\n",
    "        # 'datetime_ch_start_to_datetime_ch_end_milliseconds',\n",
    "        # 'datetime_ph_start_to_datetime_ph_end_milliseconds',\n",
    "        # 'datetime_pl_start_to_datetime_pl_end_milliseconds',\n",
    "        'reject'\n",
    "    ]  \n",
    "\n",
    "    cleaned_station_df = all_features\n",
    "    selected_df = df[columns_to_merge]\n",
    "\n",
    "    # Ensure the indices are aligned if necessary\n",
    "    updated_station_df = cleaned_station_df.reset_index(drop=True)\n",
    "    selected_df = selected_df.reset_index(drop=True)\n",
    "\n",
    "    # Merge the selected columns with cleaned_df\n",
    "    final_station_df = pd.concat([updated_station_df, selected_df], axis=1)\n",
    "    \n",
    "    return final_station_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6246e0d-9f93-4f36-bbcf-0b967c62250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████████████████████████████████████████████████████████| 30/30 [04:34<00:00,  9.14s/it]\n",
      "Feature Extraction: 100%|██████████████████████████████████████████████████████████████| 30/30 [03:44<00:00,  7.48s/it]\n",
      "Feature Extraction: 100%|██████████████████████████████████████████████████████████████| 30/30 [03:38<00:00,  7.29s/it]\n",
      "Feature Extraction: 100%|██████████████████████████████████████████████████████████████| 30/30 [04:11<00:00,  8.39s/it]\n"
     ]
    }
   ],
   "source": [
    "station1_df = process_station(df1, station_id=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "137f86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "station1_df.to_csv('final_station1_df_mover_full_.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a722cc8-d70b-45a8-8099-abef3628d120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
